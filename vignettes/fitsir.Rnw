%\VignetteEngine{knitr::knitr}
%\VignetteDepends{ggplot2}
%\VignetteDepends{plyr}
%\VignetteDepends{reshape2}
%\VignetteIndexEntry{Simple SIR model fitting}
\documentclass{article}
\title{Basic SIR fitting}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{hyperref}
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\code}[1]{{\tt #1}}
\bibliographystyle{chicago}
\date{\today}
\begin{document}
\maketitle

<<opts,echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.width=5,fig.height=5,tidy=FALSE,message=FALSE,error=FALSE,warning=FALSE)
@

\section{Harbin}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4in]{Dietz_harbin_sm.png}
\end{center}
\caption{Unnumbered figure (p. 102) from \cite{dietz_epidemics:_2009} showing the Harbin epidemic.}
\label{fig:dietzfig}
\end{figure}

Figure~\ref{fig:dietzfig} shows a Kermack-Mckendrick model fit to Harbin plague data. Based on the equations  (\ref{eq:km}) and estimates (``$x_0 = 2985$, $\rzero=2.00$ and
a mean infectious period of 11 days'') that \cite{dietz_epidemics:_2009} provides, we can compare how Kermack-Mckendrick model fit differs from SIR model fit based on maximum likelihood estimation.
\begin{equation}
\begin{split}
\frac{dz}{dt} & = \frac{\gamma x_0}{2 \rzero^2} c_1 \text{sech}^2(c_1 \gamma t - c_2) , \\
c_1 & = \sqrt{(\rzero-1)^2 + \frac{2 \rzero^2}{x_0}} \\
c_2 & = \text{tanh}^{-1} \left(\frac{\rzero-1}{c_1}\right).
\label{eq:km}
\end{split}
\end{equation}
We note that the original equation provided by \cite{dietz_epidemics:_2009} contains a typo. $c_1 \gamma t$ after $\text{sech}^2$ in the first equation should be corrected to $c_1 \gamma t/2$ \citep{kermack1927contribution}.

First, load the pacakge: 
<<load>>==
library(fitsir)
@

\noindent 
Since \code{fitsir} package lazy loads all data, \code{data(harbin)} is unnecessary.
<<harbin_head>>==
head(harbin)
@

Then, we transform the parameters provided by \cite{dietz_epidemics:_2009} into \emph{unconstrained parameters} (\code{log.beta, log.gamma, log.N, logit.i}) so that they can be used as starting parameters for MLE. Although \code{fitsir} expects a dataframe with column names \code{times} and \code{count}, we can specify a time column and a count column with \code{tcol} and \code{icol} arguments.

<<harbin_fit>>==
dietz_harbin <- c(x0=2985,rzero=2,gamma=7/11)
dietz_lpars <- with(as.list(dietz_harbin),
      c(log.beta=log(rzero*gamma),
        log.gamma=log(gamma),
        log.N=log(x0),
        logit.i=qlogis(1e-3)))
(ff <- fitsir(harbin, start=dietz_lpars, type="death", 
              tcol="week", icol="Deaths", method="BFGS"))
@

\noindent 
In this case, BFGS method has been used because using sensitivity equations allows for more accurate computation of the Hessian matrix. 

We can plot \code{fitsir} objects using \code{plot} function to see whether this fit is good or not (\code{plot(ff)}). Here, we plot SIR fit along with Dietz fit to compare how they differ:
<<harbin_plot, echo = TRUE, message = FALSE, fig.height=4>>=
plot(ff, main="SIR vs. KM comparison")
times <- with(as.list(harbin), seq(min(week), max(week), by = 0.1))
dpKM <- with(as.list(dietz_harbin), 
        {
           c1 <- sqrt((rzero-1)^2+2*rzero^2/x0)
           c2 <- atanh((rzero-1)/c1)
           gamma*x0/(2*rzero^2)*c1*
               (1/cosh(c1*gamma*times/2-c2))^2
        })
lines(times,dpKM, col = 2)
legend(x=2, y=275, legend=c("SIR","Dietz"), col=c("black", "red"), lty = 1)
@

\noindent
Apart from the differences in the estimated trajectories, we note that the Kermack-Mckendrick equation models the instantaneous change in the number of recovered individuals ($dR/dt$) whereas \code{fitsir} fits are based on the actual number of individuals that recovered during a given time interval ($R(\tau_{n+1}) - R(\tau_n)$).

We can also use the \code{summary} method provided by the \code{fitsir} package to see the summarized parameters:
<<harbin_summary>>=
summary(ff)
@

\noindent
MLE returns slightly higher $\rzero$ and longer infectious period but lower population size.

\subsection{Overdispersion}

In fact, this is not the best fit. By looking at the sum of Pearson residuals divided by the mean (given by \code{sigma}), we can see that the data is over dispersed

<<harbin_test>>==
sigma(ff)
@

\noindent
\code{fitsir} provides three ways of dealing with overdispersion (quasipossion, NB1, NB2) and in this case, using NB1 error function fits better (higher Log-likelihood) than using any of the provided error functions. First, to explore how these fits differ, we define a new data frame, namely \code{harbin2}, to avoid using \code{tcol} and \code{icol} arguments:

<<harbin2>>==
harbin2 <- setNames(harbin, c("times", "count"))
@

\noindent
Then, we can fit:

<<harbin_dispersion>>=
ff2 <- fitsir(harbin2, dist="quasipoisson", type="death", method="BFGS", start=dietz_lpars)
ff3 <- fitsir(harbin2, dist="nbinom", type="death", method="BFGS", start=dietz_lpars)
ff4 <- fitsir(harbin2, dist="nbinom1", type="death", method="BFGS", start=dietz_lpars)
@

\noindent
If you run the code, you will notice that \code{nbinom} and \code{nbinom1} estimate 5 parameters rather than 4. The last parameter \code{log.dsp} is the log of dispersion parameter. For NB2 (\code{nbinom}), \code{log.dsp} is log of the size parameter, whereas for NB1 (\code{nbinom1}), \code{log.dsp} is the log of $\tau$, where $\mu = \tau \mathrm{Var}$.

Again, we can plot these three fits to compare:

<<harbin_dispersion_plot, fig.width=8>>==
plot(ff2, level=0.95, col.traj="green", col.conf="green", log="y", main="Comparison of three error functions")
plot(ff3, level=0.95, add=TRUE, col.traj="blue", col.conf="blue")
plot(ff4, level=0.95, add=TRUE, col.traj="red", col.conf="red")
legend(x=2, y=275, legend=c("Qausipoisson","NB2", "NB1"), col=c("green", "blue", "red"), lty = 1)
@

All these three fits give us very similar expected trajectories as well as confidence intervals. However, if we compare their log-likelihoods, we find that NB1 gives us the best fit.

<<harbin_logLik>>==
hfits <- list(QP=ff2, NB2=ff3, NB1=ff4)
lapply(hfits, logLik)
@

To understand why NB1 fits better than NB2, we can look at the mean variance relationship (we disregard quasipoisson due to its high log likelihood value).

<<mean-variance, echo=FALSE>>==
library(dplyr)
library(ggplot2); theme_set(theme_bw())
mvrel <- function(fit, data) {
    mean <- SIR.detsim(data$times, coef(fit, "trans"), type="death")
    data.frame(
        mean=mean,
        var=(data$count-mean)^2
    )
}
level <- seq(0, 300, by = 25)

mvfun <- . %>%
    mvrel(harbin2) %>%
    mutate(group=cut(mean, breaks=level)) %>%
    group_by(group) %>%
    summarise(mean2 = mean(mean), var2=mean(var), n=length(var))

mvtot <- hfits %>%
    lapply(mvfun) %>%
    bind_rows(.id="dist") %>%
    filter(dist != "QP")

nb1k <- sigma(ff4, "nbinom1")
nb2k <- sigma(ff3, "nbinom")

ggplot(mvtot, aes(mean2, var2)) + 
    geom_point(aes(size=n, col=dist), pch=1) +
    scale_x_continuous(lim=c(0, 250), expand = c(0,0), name="mean") +
    scale_y_continuous(lim=c(0, 600), expand = c(0, 0), name="variance") +
    scale_size_continuous(range = c(5, 20), guide=FALSE) +
    geom_abline(intercept=0,slope=nb1k) + 
    stat_function(fun=function(x) x + x^2*nb2k, linetype=2)
@

\noindent
Clearly, we can see that the quadratic mean-variance relationship is not appropriate in this case. 

Summarizing the best fit, we underestimate $\rzero$ as well as the population size but the estimate of the infectious period is very close to that provided by Dietz.

<<harbin_nb1sum>>==
summary(ff4)
@

\subsection{Autocorrelation}

We notice that all three models (quasipoisson, NB1, and NB2) provide extremely similar trajectories as well as confidence intervals. So we can test for auto correlation:

<<harbin_acf>>=
acf(residuals(ff4, "raw"))
@
This doesn't seem like they're autocorrelated...
\section{1918 Philadelphia Flu}

Another data set provided by the \code{fitsir} package is 1918 philadelphia flu data.

<<phila_head>>==
head(phila1918)
@

Notice that the first column is in the \code{Date} format. Since \code{fitsir} expects a time column to be a numeric vector, we can either add a new column or create a new data frame. Here, we create a new data frame to avoid using \code{tcol} and \code{icol} argument.

<<phila_newdata>>==
phila1918a <- with(phila1918, data.frame(times=seq_along(date), count=pim)) 
plot(phila1918a, log="y")
@

Notice that this data doesn't exactly follow the typical SIR trajectory. Due to its long tail, most models other than Gaussian fail. First, we can try to fit SIR model naively:

<<phila_fit>>==
(pfit <- fitsir(phila1918a, type="death"))
plot(pfit, log="y")
@

We get a decent fit. However, this is actually a local minima. To avoid falling in a local minima, we can either use a different starting point or try different method. Here, we present how using different starting values can yield different results.

\subsection{Starting values}

\code{fitsir} provides a function (\code{startfun}) that automatically finds a reasonable starting value:

<<phila_start>>==
(pstart <- startfun(phila1918a, type="death"))
plot(phila1918a, log="y")
lines(SIR.detsim(phila1918a$times, trans.pars(pstart), type="death"))
@

Using this starting function, we can get a better fit:

<<phila_start2>>==
(pfit2 <- fitsir(phila1918a, type="death", start=pstart))
@

Yet, this is still not the best fit. We can further explore different starting values using Latin hypercube sampling.

<<phila_LHS>>===
set.seed(123)
lhsfun <- function(param, size=0.5, length.out=20) {
    seq((1-size)*param, (1+size)*param, length.out=length.out)
}

ltab <- sapply(pstart, lhsfun)
ltab <- apply(ltab,2, sample)
plist <- apply(ltab, 1, function(x) fitsir(phila1918a, type="death", start=x))
(pLik <- sapply(plist, logLik))
@

We can explore the parameter space

<<phila_param>>==
ppars <- as.data.frame(t(sapply(plist, coef)))
col <- c("black", "blue", "red")
ccol <- col[cut(pLik, breaks=c(-900, -600, -520, -500))]

plot(ppars, col=ccol)
@

We can see that the best fits (red points) converge to a single point. We get some fits that are not optimal but are close to the best fits (blue points). Then, there are fits that fail (black points).

<<phila_traj>>==
ppred <- plist %>%
    lapply(predict) %>%
    bind_rows(.id="sim") 

ppred$logLik <- rep(pLik, each=length(phila1918a$times))

ggplot(ppred) +
    geom_line(aes(times, mean, group=sim, col=logLik)) +
    geom_point(data=phila1918a, aes(times, count))
@

Looking at the best fit...

<<phila_best>>===
pbest <- plist[[which.max(pLik)]]
summary(pbest) ## Nelder-Mead being unstable
pbest2 <- fitsir(phila1918a, type="death", start=coef(pbest), method="BFGS")
summary(pbest2)
@

\section{Bombay}

In this section, we are going to be demonstrating some pathologies that might be associated with SIR fitting.

<<bombay_set>>==
bombay2 <- setNames(bombay, c("times", "count"))
bb <- fitsir(bombay2, type="death", dist="nbinom", start=startfun(bombay2, type="death"))
@

Multiple local minimas:

<<bombay_lm>>==
blist <- apply(ltab, 1, function(x) fitsir(bombay2, type="death", start=x, method="BFGS"))
bpars <- as.data.frame(t(sapply(blist, coef)))
bLik <- sapply(blist, logLik)
mle <- max(bLik)

goodfits <- which(1.001 * mle < bLik)

gpars <- bpars[goodfits,]

plot(gpars)
@

Difference is not noticeable:

<<bombay_lm_traj>>==
plot(bombay2)
l <- lapply(blist[goodfits], function(x) plot(x, add=TRUE))
@

Autocorrelation:

<<bombay_acf>>==
acf(residuals(bb, "raw"))
@

\bibliography{plague}
\end{document}
