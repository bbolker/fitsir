%\VignetteEngine{knitr::knitr}
%\VignetteDepends{ggplot2}
%\VignetteDepends{plyr}
%\VignetteDepends{reshape2}
%\VignetteIndexEntry{Simple SIR model fitting}
\documentclass{article}
\title{Basic SIR fitting}
\author{Ben Bolker, David Earn, Dora Rosati}
\usepackage{amsmath}
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\code}[1]{{\tt #1}}
\date{\today}
\begin{document}
\maketitle

This has been done a million times, but let's try to do it in 
a reasonably systematic way that could be used in a pedagogical paper.

<<opts,message=FALSE,echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.width=4,fig.height=4)
knit_hooks$set(baseFig=function(before, options, envir) {
                   if (before) {
                       ## tweak graphical settings for base figures
                       par(bty="l",las=1)
                   } else { }
               })
@

<<pkgs,message=FALSE>>=
library("fitsir")
library("bbmle") ## need this for now, for coef()
library("plyr")
library("reshape2")
library("ggplot2"); theme_set(theme_bw())
@

The current version of \code{fitsir} assumes that time
and prevalence are stored as columns \code{tvec} and \code{count}
within a data frame.  Since the \code{bombay} data set instead
has \code{week} (week of epidemic) and \code{mort} (mortality),
we'll rename it for convenience.  (We will for now resolutely ignore
issues about fitting weekly mortality counts as prevalences \ldots)

<<rename>>=
bombay2 <- setNames(bombay,c("tvec","count"))
@

<<plot1,baseFig=TRUE>>=
plot(count~tvec,data=bombay2,
     type="l",xaxs="i",yaxs="i",
     xlab="time",ylab="mortality count")
@


\section{Fit the model to the data}

Basic fit:
<<fit1,cache=TRUE>>=
m1 <- fitsir(data=bombay2)
@

<<>>=
summarize.pars(coef(m1))
@

Seemingly reasonable answers, but ...
<<plotres1,baseFig=TRUE>>=
ss <- with(bombay2,SIR.detsim(tvec,trans.pars(coef(m1))))
plot(count~tvec,data=bombay2,
     xlab="time",ylab="mortality count")
lines(bombay2$tvec,ss,col=2)
@

\section{Troubleshooting}

We're obviously not getting a good answer here.  When this happens there
are a variety of possibilities.

\begin{itemize}
\item optimizer getting stuck
\item a small number of local optima
\item a large number of local optima, on many different scales (fractal-like or
  rugged surface)
\item a large number of local optima, all similar in scale/height (``fakir's bed'' geometry)
\end{itemize}

Some solutions:

\begin{itemize}
\item center/scale parameters and/or 
  reparameterize the model to remove correlation and equalize scales of variation in 
  different parameters
\item try to come up with a rule for finding better starting values (``self-starting'' fits)
\item use a better/more robust local optimizer
\item use lots of starting values, randomly or regularly distributed
\item use a stochastic global optimizer
\end{itemize}

<<>>=
confint(m1,method="quad")
@

Suggests \emph{some} sort of unidentifiability \ldots

What if we try a bunch of starting values?

A crude Latin-hypercube-like strategy: pick evenly spaced
values on sensible log scales, then permute to get random 
(but even) coverage of the space.

<<qlhc>>=
qlhcfun <- function(n=5,seed=NULL) {
    require("plyr")
    if (!is.null(seed)) set.seed(seed)
    R0vec <- 1+10^seq(-1,1.5,length=n)
    infpervec <- sample(10^seq(-1,2,length=n))
    Nvec <- sample(10^seq(2,5,length=n))
    i0vec <- sample(10^seq(-3,-1,length=n))
    startlist <- alply(cbind(R0=R0vec,infper=infpervec,N=Nvec,i0=i0vec),1,
                       function(x) {
                           with(as.list(x), {
                               beta <- R0/infper
                               gamma <- 1/infper
                               c(log.beta=log(beta),log.gamma=log(gamma),
                                 log.N=log(N),logit.i=qlogis(i0))
                           })
                       })
    return(startlist)
}
startlist <- qlhcfun(n=5,seed=101)
@

<<fitlhc5,cache=TRUE>>=
fitlist <- llply(startlist,fitsir,data=bombay2,
      method="Nelder-Mead",control=list(maxit=1e5))
@

<<plotlhc5>>=
## extract log-likelihoods
likframe <- data.frame(.id=1:5,llik=unlist(llply(fitlist,logLik)))
## compute trajectories
gettraj <- function(x,tvec=bombay2$tvec) {
    data.frame(tvec=tvec,
               count=SIR.detsim(tvec,trans.pars(coef(x))))
}
fittraj <- ldply(fitlist,gettraj)
fittraj <- merge(fittraj,likframe)
## plot together
ggplot(fittraj,aes(tvec,count,colour=llik,group=.id))+geom_line()+
    geom_point(data=bombay2,colour="black",aes(group=NA))
@

Now try a much larger sample:

<<fitlhc100,cache=TRUE,results="hide">>=
startlist100 <- qlhcfun(n=100,seed=101)
fitlist100 <- llply(startlist100,
                    function(x) {
                        r <- try(fitsir(start=x,data=bombay2),silent=TRUE)
                        if (is(r,"try-error")) NULL else r
                    })
@

<<lhc100coefs,message=FALSE>>=
testOK  <- function(x,max.R0=100,max.r=1000,max.infper=400) {
    if (is.null(x)) return(FALSE)
    ss <- summarize.pars(coef(x))
    return(ss["R0"]<max.R0 & ss["r"]<max.r & ss["infper"] < max.infper)
}
fitlist100.OK <- fitlist100[sapply(fitlist100,testOK)]
length(fitlist100.OK)
fittab <- laply(fitlist100.OK,function(x) c(summarize.pars(coef(x)),logLik(x)))
ggplot(melt(fittab),aes(x=value))+geom_histogram()+facet_wrap(~Var2,scale="free")
@

<<plot100,message=FALSE>>=
likframe100 <- setNames(ldply(fitlist100.OK,logLik),c(".id","llik"))
fittraj100 <- ldply(fitlist100.OK,gettraj,tvec=seq(1,32,length=101))
fitmat100 <- acast(fittraj100,tvec~.id,value.var="count")
fittraj100 <- merge(fittraj100,likframe100)
## plot together
ggplot(fittraj100,aes(tvec,count,colour=llik,group=.id))+geom_line()+
    geom_point(data=bombay2,colour="black",aes(group=NA))
@

We can identify clusters \ldots
<<clust>>=
clust <- kmeans(t(fitmat100),5)
cframe <- data.frame(.id=names(clust$cluster),clust=clust$cluster)
fittraj100B <- transform(fittraj100,llikcat=cut_number(llik,5))
fittraj100B <- merge(fittraj100B,cframe)
ggplot(fittraj100B,aes(tvec,count,colour=factor(clust),group=.id))+geom_line()+
    geom_point(data=bombay2,colour="black",aes(group=NA))+
        scale_colour_brewer(palette="Paired")
@

Check out clustering on log-likelihood cumulative distribution curve:
<<clust2>>=
dd2 <- merge(cframe,likframe100)
(g1 <- ggplot(dd2,aes(rank(llik),llik,colour=factor(clust)))+geom_point(size=2)+
             scale_colour_brewer(palette="Paired"))
@

I'm not 100\% sure (yet) what this tells us.  The clusters aren't so
well separated that I necessarily believe that they are distinct modes.

<<>>=
startmat100 <- do.call(rbind,startlist100)
@

\textbf{to do: characterize starting value sets by cluster (or ``bad''),
  plot, look for regularities.  Suspect that large $i_0$ is a problem?}

\section{Self-starting strategies}

Try \code{smooth.spline} with \code{spar=0.5} to identify max.; linear regression
through times up to 1/2 tmax to identify $i(0) N$ and $r$; then try a range of other
parameters?

<<ssplot,baseFig=TRUE>>=
tvec <- bombay2$tvec
ss <- with(bombay2,smooth.spline(tvec,log(count),spar=0.5))
ss.tmax <- uniroot(function(x) predict(ss,x,deriv=1)$y,c(0,40))$root
plot(log(count)~tvec,data=bombay2)
lines(predict(ss,tvec))
abline(v=ss.tmax,lty=2)
ss.thalf <- min(tvec)+(ss.tmax-min(tvec))/2
@

<<linquadfits,baseFig=TRUE>>=
plot(log(count)~tvec,data=bombay2,
     xlab="time",ylab="mortality count")
bestFit <- fitlist100.OK[[which.max(likframe100$llik)]]
bestTraj <- gettraj(bestFit)
m1 <- lm(log(count)~tvec,data=subset(bombay2,tvec<ss.thalf))
c(linfit=coef(m1)[2],
  sirfit=with(as.list(coef(bestFit)),exp(log.beta)-exp(log.gamma)))
with(bestTraj,lines(log(count)~tvec,col=2))
abline(m1)
m4 <- lm(log(count)~poly(tvec,2,raw=TRUE),data=subset(bombay2,tvec>10 & tvec<25))
lines(11:24,predict(m4),col=6,lwd=2)
@

Looks like this works.  Is there a way to get a crude starting guess for $R_0$ and
$N$?

Quadratic fit to peak of log trajectory is very good: 
what do these parameters tell us about the epidemic?

We want $d^2(\log I)/dt^2$ at the peak \ldots
$d \log I/dt=\beta S/N- \gamma$ and $\hat S= \gamma N/\beta$ so we have 
(at the peak where $d \log I/dt=0$)
\begin{equation}
d^2 \log I/dt^2 =
\beta S'/N = \beta(-\beta SI/N)/N = -\beta^2/N^2(\gamma N/\beta)I =-\beta \gamma I/N
\end{equation}

Second derivative of $a + bt + ct^2= 2c$

<<fitstats>>=
Qp <- unname(2*coef(m4)[3])
Qp.alt <- predict(ss,ss.tmax,deriv=2)$y
Ip <- max(predict(m4))  ## peak log(I) by smoothing
(Qfits <- c(quadfit=-Qp,ssderiv2=-Qp.alt,
  sirfit=with(as.list(coef(bestFit)),exp(log.beta+log.gamma+Ip-log.N))))
@
OK, I guess, although I expected a little better?
Second derivative at the smoothing spline peak is a little bit easier
(we don't have to decide on a range over which to fit the quadratic),
and in this case is actually closer to the theoretical value
(proportional error 
\Sexpr{round(Qfits["ssderiv2"]/Qfits["sirfit"]-1,2)} vs.
\Sexpr{round(Qfits["quadfit"]/Qfits["sirfit"]-1,2)})
although it's possibly more sensitive to weird shapes at the peak.

This should get us one more parameter.

With $Q$, $a_0$, and $b_0$, this gives me so far:

\begin{equation}
\begin{split}
  a_0 & = \log i_0 + \log N \quad \textrm{(initial number infected, log scale)} \\
  b_0 & = \log \beta - \log \gamma \quad (r) \\
  \log(-Q)-\log(I_{\textrm{max}})  & = \log \beta + \log \gamma + \log N
\end{split}
\end{equation}

$S=\gamma N/\beta$ at the peak time is approximately $N-I_0-\sum_{t=0}^{\hat t} S(t)$
(we have to be careful to decide whether we're counting incidence or prevalence, and correct
for $\gamma$ accordingly: $\textrm{prevalence} = \textrm{incidence}/\gamma$).

Number of counts up to peak:
<<>>=
sumcount.tmax <- with(subset(bombay2,tvec<ss.tmax),sum(count))
@
Should be able to do something with this??

Should use $N (1-i_0)$, not $N$, as starting condition for $S$.

\end{document}
