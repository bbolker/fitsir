%\VignetteEngine{knitr::knitr}
%\VignetteDepends{ggplot2}
%\VignetteDepends{plyr}
%\VignetteDepends{dplyr}
%\VignetteDepends{reshape2}
%\VignetteIndexEntry{Simple SIR model fitting}
\documentclass{article}
\title{Basic SIR fitting}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{hyperref}
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\code}[1]{{\tt #1}}
\bibliographystyle{chicago}
\date{\today}
\begin{document}
\maketitle

<<opts,echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.width=5,fig.height=5,tidy=FALSE,message=FALSE,error=FALSE,warning=FALSE)
@

\section{Preliminaries}

Load packages:

<<pkgs,message=FALSE>>=
library(fitsir)
library(dplyr)
library(ggplot2); theme_set(theme_bw())
@ 

\section{Harbin}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4in]{Dietz_harbin_sm.png}
\end{center}
\caption{Unnumbered figure (p. 102) from \cite{dietz_epidemics:_2009} showing the Harbin epidemic.}
\label{fig:dietzfig}
\end{figure}

Figure~\ref{fig:dietzfig} shows a Kermack-McKendrick model fit to Harbin plague data. Based on the equations (\ref{eq:km}) below and estimates (``$x_0 = 2985$, $\rzero=2.00$ and
a mean infectious period of 11 days'') that \cite{dietz_epidemics:_2009} provides, we can compare the Kermack-McKendrick model to an SIR model fit based on maximum likelihood estimation.
\begin{equation}
\begin{split}
\frac{dz}{dt} & = \frac{\gamma x_0}{2 \rzero^2} c_1 \text{sech}^2(c_1 \gamma t - c_2) , \\
c_1 & = \sqrt{(\rzero-1)^2 + \frac{2 \rzero^2}{x_0}} \\
c_2 & = \text{tanh}^{-1} \left(\frac{\rzero-1}{c_1}\right).
\label{eq:km}
\end{split}
\end{equation}
\footnote{The original equation provided by \cite{dietz_epidemics:_2009} contains a typo. $c_1 \gamma t$ after $\text{sech}^2$ in the first equation should be corrected to $c_1 \gamma t/2$ \citep{kermack1927contribution}.}

The \code{harbin} data set is built in:
<<harbin_head>>==
head(harbin)
@

We can transform the parameters provided by \cite{dietz_epidemics:_2009} into \emph{unconstrained parameters} (\code{log.beta}, \code{log.gamma}, \code{log.N}, \code{logit.i}) so that they can be used as starting parameters for \code{fitsir}. Although \code{fitsir} expects a dataframe with column names \code{times} and \code{count}, we can specify a time column and a count column with \code{tcol} and \code{icol} arguments.

<<harbin_fit>>==
dietz_harbin <- c(x0=2985,rzero=2,gamma=7/11)
dietz_lpars <- with(as.list(dietz_harbin),
      c(log.beta=log(rzero*gamma),
        log.gamma=log(gamma),
        log.N=log(x0),
        logit.i0=qlogis(1e-3)))

(ff <- fitsir(harbin, start=c(trans.pars(dietz_lpars), sigma=100),
              type="death", 
              tcol="week", icol="Deaths"))
@

We can plot \code{fitsir} objects using the \code{plot} function to judge goodness of fit (\code{plot(ff)}). Here, we plot the SIR fit along with the Dietz fit:
<<harbin_plot, echo = TRUE, message = FALSE, fig.height=4>>=
plot(ff, main="SIR vs. KM comparison")
times <- harbin$week
dpKM <- with(as.list(dietz_harbin), 
        {
           c1 <- sqrt((rzero-1)^2+2*rzero^2/x0)
           c2 <- atanh((rzero-1)/c1)
           gamma*x0/(2*rzero^2)*c1*
               (1/cosh(c1*gamma*times/2-c2))^2
        })
lines(times,dpKM, col = 2)
legend(x=2, y=275, legend=c("SIR","Dietz"), col=c("black", "red"), lty = 1)
@

\noindent
Apart from the difference in the estimated trajectories, we note that the Kermack-Mckendrick equation models the \emph{instantaneous} change in the number of recovered individuals ($dR/dt$) whereas \code{fitsir} models the changes in the number of recovered individuals between a time interval ($R(\tau_{n+1}) - R(\tau_n)$).

After fitting, we can use the \code{summary} method to see the summarized parameters:
<<harbin_summary>>=
summary(ff)
@

\noindent
Note that \code{fitsir} estimates slightly higher $\rzero$ and longer infectious period but lower population size. Here, standard errors are calculated by using the Delta method (see \code{vignette("details", package="fitsir")}).

\subsection{Overdispersion}

In fact, this is not the best fit. By looking at the sum of Pearson residuals divided by the predicted mean (given by \code{dispersion}), we can see that the data is over dispersed (we expect this value to be approximately 1)

<<harbin_test>>==
dispersion(ff)
@

\noindent
\code{fitsir} provides three ways of dealing with overdispersion (quasipossion, NB1, NB2). In this case, using NB1 error function fits better (higher Log-likelihood) than using any of the provided error functions. 

First, to explore how these fits differ, we define a new data frame, namely \code{harbin2}, to avoid using \code{tcol} and \code{icol} arguments:

<<harbin2>>==
harbin2 <- setNames(harbin, c("times", "count"))
@

\noindent
Then, we can fit:

<<harbin_dispersion>>=
ff2 <- fitsir(harbin2, dist="quasipoisson", type="death", start=dietz_lpars)
ff3 <- fitsir(harbin2, dist="nbinom", type="death", start=c(dietz_lpars, ll.k=5))
ff4 <- fitsir(harbin2, dist="nbinom1", type="death", start=c(dietz_lpars, ll.phi=5))
@

\noindent
Notice that \code{nbinom} and \code{nbinom1} estimate 5 parameters rather than 4. The last parameters are the log-transformed dispersion parameters. 
NB2 (\code{nbinom}) assumes quadratic mean-variance relationship: $\mathrm{Var} = \mu (1 + \mu/k)$. In this case, we estimate \code{ll.k=exp(k)} instead.
On the other hand, NB1 (\code{nbinom1}) assumes linear mean-variance relationship: $\mathrm{Var}=\mu (1+\phi)$. In this case, we estimate \code{ll.phi=exp(phi)} instead.

Again, we can plot these three fits along with their confience intervals (\code{level=0.95}) to compare:

<<harbin_dispersion_plot, fig.width=8>>==
plot(ff2, level=0.95, col.traj="green", col.conf="green", log="y", main="Comparison of three error functions")
plot(ff3, level=0.95, add=TRUE, col.traj="blue", col.conf="blue")
plot(ff4, level=0.95, add=TRUE, col.traj="red", col.conf="red")
legend(x=2, y=275, legend=c("Qausipoisson","NB2", "NB1"), col=c("green", "blue", "red"), lty = 1)
@

All these three fits give us very similar expected trajectories as well as confidence intervals. However, if we compare their AIC valuess, we find that NB1 gives us the best fit.

<<harbin_logLik>>==
hfits <- list(QP=ff2, NB2=ff3, NB1=ff4)
lapply(hfits, AIC)
@

To understand why NB1 fits better than NB2, we can look at the mean variance relationship (we disregard quasipoisson due to its high log likelihood/AIC value).

<<mean-variance, echo=FALSE>>==
mvrel <- function(fit, data) {
    mean <- SIR.detsim(data$times, coef(fit, "trans"), type="death")
    data.frame(
        mean=mean,
        var=(data$count-mean)^2
    )
}
level <- seq(0, 300, by = 25)

mvfun <- . %>%
    mvrel(harbin2) %>%
    mutate(group=cut(mean, breaks=level)) %>%
    group_by(group) %>%
    summarise(mean2 = mean(mean), var2=mean(var), n=length(var))

mvtot <- hfits %>%
    lapply(mvfun) %>%
    bind_rows(.id="dist") %>%
    filter(dist != "QP")

## `dispersion` returns raw dispersion parameters that have not been transformed
nb1k <- dispersion(ff4, "nbinom1")
nb2k <- dispersion(ff3, "nbinom")

ggplot(mvtot, aes(mean2, var2)) + 
    geom_point(aes(size=n, col=dist), pch=1) +
    scale_x_continuous(lim=c(0, 250), expand = c(0,0), name="mean") +
    scale_y_continuous(lim=c(0, 600), expand = c(0, 0), name="variance") +
    scale_size_continuous(range = c(5, 20), guide=FALSE) +
    geom_abline(intercept=0,slope=nb1k+1) + 
    stat_function(fun=function(x) x + x^2/nb2k, linetype=2)
@

\noindent
Clearly, we can see that the quadratic mean-variance relationship is not appropriate in this case. 

Summarizing the best fit, we underestimate $\rzero$ as well as the population size but the estimate of the infectious period is very close to that provided by Dietz.

<<harbin_nb1sum>>==
summary(ff4)
@

\subsection{Autocorrelation}

All three models (quasipoisson, NB1, and NB2) provide extremely similar trajectories as well as confidence intervals. So we can test for auto correlation:

<<harbin_acf>>=
acf(residuals(ff4, "raw"))
@
This doesn't seem like they're autocorrelated...

\section{1918 Philadelphia Flu}

Another data set provided by the \code{fitsir} package is 1918 philadelphia flu data. In this section, we are going to show how using different optimizers and starting values can affect the fit.

<<phila_head>>==
head(phila1918)
@

Notice that the first column is in the \code{Date} format. Since \code{fitsir} expects a time column to be a numeric vector, we can either add a new column or create a new data frame. Here, we create a new data frame to avoid using \code{tcol} and \code{icol} argument.

<<phila_newdata>>==
phila1918a <- with(phila1918, data.frame(times=seq_along(date), count=pim)) 
plot(phila1918a, log="y")
@

\noindent
Notice that this data doesn't exactly follow the typical SIR trajectory. Due to its long tail, most models other than Gaussian fail. 

For the Harbin data, we used Dietz parameter as a starting parameter but we can't do that here. First, we can try to fit an SIR model using arbitrary startin values. By default, \code{fitsir} uses \code{nlminb} but for sake of demonstration, we are going to use the Nelder-Mead method (see \code{?optim}).

<<phila_fit>>==
(pfit <- fitsir(phila1918a, 
    start=c(log.beta=log(0.12), log.gamma=log(0.09), log.N=log(10000), logit.i=qlogis(0.01)),  
    type="death",
    method="Nelder-Mead",
    optimizer="optim")
)
plot(pfit)
@

We get a decent fit. However, this is actually not the maximum likelihood estimate. To avoid falling in a local minima, we can either use a different starting point or try different optimization methods. Here, we present how using different starting values can yield different results.

\subsection{Starting values}

\code{fitsir} provides a function (\code{startfun}) that automatically finds a reasonable starting value:

<<phila_start>>==
(pstart <- startfun(phila1918a, type="death"))
plot(phila1918a, log="y")
lines(SIR.detsim(phila1918a$times, trans.pars(pstart), type="death"))
@

Using this starting value, we can get a better fit:

<<phila_start2>>==
(pfit2 <- fitsir(phila1918a, type="death", start=pstart, method="Nelder-Mead", optimizer="optim"))
@

We can further explore local minima using Latin hypercube sampling.

<<phila_LHS,cache=TRUE, warning=FALSE>>===
set.seed(123)
lhsfun <- function(param, size=0.5, length.out=20) {
    seq((1-size)*param, (1+size)*param, length.out=length.out)
}

ltab <- sapply(pstart, lhsfun)
ltab <- apply(ltab,2, sample)
plist <- apply(ltab, 1, fitsir,
    data=phila1918a, type="death", method="Nelder-Mead", optimizer="optim")

(pLik <- sapply(plist, logLik))
@

We can explore the parameter space:

<<phila_param>>==
ppars <- as.data.frame(t(sapply(plist, coef)))
col <- c("black", "blue", "red")
ccol <- col[cut(pLik, breaks=c(-900, -700, -520, -500))]

plot(ppars, col=ccol)
@

We can see that the best fits (red points) converge to a single point. We get some fits that are not optimal but are close to the best fits (blue points). Then, there are fits that fail (black points).

<<phila_traj>>==
ppred <- plist %>%
    lapply(predict) %>%
    bind_rows(.id="sim") 

ppred$logLik <- rep(pLik, each=length(phila1918a$times))

ggplot(ppred) +
    geom_line(aes(times, mean, group=sim, col=logLik)) +
    geom_point(data=phila1918a, aes(times, count))
@

Looking at the best fit...

<<phila_best>>===
pbest <- plist[[which.max(pLik)]]
summary(pbest) ## Nelder-Mead being unstable
pbest2 <- fitsir(phila1918a, type="death", start=coef(pbest), method="BFGS")
summary(pbest2)
@

Note that we used the \code{Nelder-Mead} method only for sake of demonstration. Gradient based methods such as \code{nlminb} or \code{BFGS} can still get stuck in local minima as well but are more robust.

\section{Bombay}

In this section, we are going to be demonstrating some pathologies that might be associated with fitting an SIR model.

<<bombay_set>>==
bombay2 <- setNames(bombay, c("times", "count"))
bbstart <- startfun(bombay2, type="death")
bb <- fitsir(bombay2, type="death", dist="nbinom", start=c(bbstart, ll.k=5))
@

When $\rzero$ is close to 1, fits may not converge properly.

Multiple local minima:

<<bombay_lm,cache=TRUE, warning=FALSE>>==
ltab <- sapply(bbstart, lhsfun)
ltab <- apply(ltab,2, sample)
blist <- apply(ltab, 1, fitsir,
               data=bombay2, type="death")
bpars <- as.data.frame(t(sapply(blist, coef)))
bLik <- sapply(blist, logLik)
mle <- max(bLik)
goodfits <- which(1.001 * mle < bLik)
gpars <- bpars[goodfits,]
plot(gpars)
@

The difference is barely noticeable:

<<bombay_lm_traj>>==
plot(bombay2)
l <- lapply(blist[goodfits], function(x) plot(x, add=TRUE))
@

Autocorrelation:

<<bombay_acf>>==
acf(residuals(bb, "raw"))
@

\bibliography{plague}
\end{document}
