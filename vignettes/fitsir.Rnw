%\VignetteEngine{knitr::knitr}
%\VignetteDepends{ggplot2}
%\VignetteDepends{plyr}
%\VignetteDepends{reshape2}
%\VignetteIndexEntry{Simple SIR model fitting}
\documentclass{article}
\title{Basic SIR fitting}
\author{Ben Bolker, David Earn, Dora Rosati}
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\code}[1]{{\tt #1}}
\date{\today}
\begin{document}
\maketitle

This has been done a million times, but let's try to do it in 
a reasonably systematic way that could be used in a pedagogical paper.

<<opts,message=FALSE,echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.width=4,fig.height=4)
knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       ## tweak graphical settings for base figures
                       par(bty="l",las=1)
                   } else { }
               })
@

<<pkgs,message=FALSE>>=
library("fitsir")
library("bbmle") ## need this for now, for coef()
library("plyr")
library("reshape2")
library("ggplot2"); theme_set(theme_bw())
@

The current version of \code{fitsir} assumes that time
and prevalence are stored as columns \code{tvec} and \code{count}
within a data frame.  Since the \code{bombay} data set instead
has \code{week} (week of epidemic) and \code{mort} (mortality),
we'll rename it for convenience.  (We will for now resolutely ignore
issues about fitting weekly mortality counts as prevalences \ldots)

<<rename>>=
bombay2 <- setNames(bombay,c("tvec","count"))
@

<<plot1,basefig=TRUE>>=
plot(count~tvec,data=bombay2,
     type="l",xaxs="i",yaxs="i",
     xlab="time",ylab="mortality count")
@


\section{Fit the model to the data}

Basic fit:
<<fit1,cache=TRUE>>=
m1 <- fitsir(data=bombay2)
@

<<>>=
summarize.pars(coef(m1))
@

Seemingly reasonable answers, but ...
<<plotres1,basefig=TRUE>>=
ss <- with(bombay2,SIR.detsim(tvec,trans.pars(coef(m1))))
plot(count~tvec,data=bombay2,
     xaxs="i",yaxs="i",
     xlab="time",ylab="mortality count")
lines(bombay2$tvec,ss,col=2)
@

\section{Troubleshooting}

We're obviously not getting a good answer here.  When this happens there
are a variety of possibilities.

\begin{itemize}
\item optimizer getting stuck
\item a small number of local optima
\item a large number of local optima, on many different scales (fractal-like or
  rugged surface)
\item a large number of local optima, all similar in scale/height (``fakir's bed'' geometry)
\end{itemize}

Some solutions:

\begin{itemize}
\item center/scale parameters and/or 
  reparameterize the model to remove correlation and equalize scales of variation in 
  different parameters
\item try to come up with a rule for finding better starting values (``self-starting'' fits)
\item use a better/more robust local optimizer
\item use lots of starting values, randomly or regularly distributed
\item use a stochastic global optimizer
\end{itemize}

<<>>=
confint(m1,method="quad")
@

Suggests \emph{some} sort of unidentifiability \ldots

What if we try a bunch of starting values?

A crude Latin-hypercube-like strategy: pick evenly spaced
values on sensible log scales, then permute to get random 
(but even) coverage of the space.

<<qlhc>>=
qlhcfun <- function(n=5,seed=NULL) {
    require("plyr")
    if (!is.null(seed)) set.seed(seed)
    R0vec <- 1+10^seq(-1,1.5,length=n)
    infpervec <- sample(10^seq(-1,2,length=n))
    Nvec <- sample(10^seq(2,5,length=n))
    i0vec <- sample(10^seq(-3,-1,length=n))
    startlist <- alply(cbind(R0=R0vec,infper=infpervec,N=Nvec,i0=i0vec),1,
                       function(x) {
                           with(as.list(x), {
                               beta <- R0/infper
                               gamma <- 1/infper
                               c(log.beta=log(beta),log.gamma=log(gamma),
                                 log.N=log(N),logit.i=qlogis(i0))
                           })
                       })
    return(startlist)
}
startlist <- qlhcfun(n=5,seed=101)
@

<<fitlhc5,cache=TRUE>>=
fitlist <- llply(startlist,fitsir,data=bombay2,
      method="Nelder-Mead",control=list(maxit=1e5))
@

<<plotlhc5>>=
## extract log-likelihoods
likframe <- data.frame(.id=1:5,llik=unlist(llply(fitlist,logLik)))
## compute trajectories
gettraj <- function(x,tvec=bombay2$tvec) {
    data.frame(tvec=tvec,
               count=SIR.detsim(tvec,trans.pars(coef(x))))
}
fittraj <- ldply(fitlist,gettraj)
fittraj <- merge(fittraj,likframe)
## plot together
ggplot(fittraj,aes(tvec,count,colour=llik,group=.id))+geom_line()+
    geom_point(data=bombay2,colour="black",aes(group=NA))
@

Now try a much larger sample:

<<fitlhc100,cache=TRUE,results="hide">>=
startlist100 <- qlhcfun(n=100,seed=101)
fitlist100 <- llply(startlist100,
                    function(x) {
                        r <- try(fitsir(start=x,data=bombay2),silent=TRUE)
                        if (is(r,"try-error")) NULL else r
                    })
@

<<lhc100coefs,message=FALSE>>=
testOK  <- function(x,max.R0=100,max.r=1000,max.infper=400) {
    if (is.null(x)) return(FALSE)
    ss <- summarize.pars(coef(x))
    return(ss["R0"]<max.R0 & ss["r"]<max.r & ss["infper"] < max.infper)
}
fitlist100.OK <- fitlist100[sapply(fitlist100,testOK)]
length(fitlist100.OK)
fittab <- laply(fitlist100.OK,function(x) c(summarize.pars(coef(x)),logLik(x)))
ggplot(melt(fittab),aes(x=value))+geom_histogram()+facet_wrap(~Var2,scale="free")
@

<<plot100,message=FALSE>>=
likframe100 <- setNames(ldply(fitlist100.OK,logLik),c(".id","llik"))
fittraj100 <- ldply(fitlist100.OK,gettraj,tvec=seq(1,32,length=101))
fitmat100 <- acast(fittraj100,tvec~.id,value.var="count")
fittraj100 <- merge(fittraj100,likframe100)
## plot together
ggplot(fittraj100,aes(tvec,count,colour=llik,group=.id))+geom_line()+
    geom_point(data=bombay2,colour="black",aes(group=NA))
@

We can identify clusters \ldots
<<clust>>=
clust <- kmeans(t(fitmat100),5)
cframe <- data.frame(.id=names(clust$cluster),clust=clust$cluster)
fittraj100B <- transform(fittraj100,llikcat=cut_number(llik,5))
fittraj100B <- merge(fittraj100B,cframe)
ggplot(fittraj100B,aes(tvec,count,colour=factor(clust),group=.id))+geom_line()+
    geom_point(data=bombay2,colour="black",aes(group=NA))
@

Check out clustering on log-likelihood cumulative distribution curve:
<<>>=
dd2 <- merge(cframe,likframe100)
(g1 <- ggplot(dd2,aes(rank(llik),llik,colour=factor(clust)))+geom_point(size=3))
@

I'm not 100\% sure (yet) what this tells us.  The clusters aren't so
well separated that I necessarily believe that they are distinct modes.
     
\end{document}
