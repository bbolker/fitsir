---
title: "Sensitivity demo"
output: html_document
---

```{r pkgs, message = FALSE}
library("deSolve")
library("bbmle")
source("../R/fitSIR_funs.R")
```

I'm going to show how to use the sensitivity functions...

## Gradient function

I'm going to claim that this is the correct sensitivity equation:

$$
\begin{aligned}
\frac{\partial}{\partial t}\frac{\partial I(t,\theta)}{\partial\theta} &= \frac{\partial f_I}{\partial\theta}\\
&= \frac{\partial f_I}{\partial S}\frac{\partial  S}{\partial\theta} + \frac{\partial f_I}{\partial I}\frac{\partial I}{\partial\theta} + \frac{\partial f_I}{\partial\theta} \\
\end{aligned}
$$

## Example 

```{r ex.sim}
tvec <- seq(1, 60, 0.2)

pars <- c(log.beta = -0.7,
	log.gamma = -2.3,
	log.N= 6.21,
	logit.i = -4.6)

pars2 <- c(log.beta = -0.69,
	log.gamma = -2.3,
	log.N= 6.21,
	logit.i = -4.6)

a = SIR.detsim(tvec, trans.pars(pars), findSens = TRUE)

a2 = SIR.detsim(tvec, trans.pars(pars2), findSens = TRUE)

z = (a2$I-a$I)/0.01

plot(a$I, type = "l")
lines(a2$I, col = "2")

plot(z, type = "l")

lines(a$nu_beta_I, col = 2)

lines(a2$nu_beta_I, col = 3)

```

## SSQ example

```{r ssq.ex}
fake.obs = as.data.frame(cbind(tvec, I=a[,c("I")]))
colnames(fake.obs) = c("tvec", "count")

pars3 <- c(log.beta = -0.7,
	log.gamma = -3.1,
	log.N= 5,
	logit.i = -2)

findSens(fake.obs, pars3, plot.it = TRUE)

pars4 <- c(log.beta = -0.7,
	log.gamma = -2.3,
	log.N= 6.21,
	logit.i = -4.6)


findSens(fake.obs, pars4)

```

## Bombay example - initial values

We start with these three equations:

$$
\begin{aligned}
r &= \beta - \gamma,\\
-\frac{Q}{I_p} &= \frac{\beta\gamma}{N},\\
N - I_0 - \int I(t) dt = \frac{\gamma}{\beta}N,
\end{aligned}
$$

where $I(t)$ is incidence. Substition gives us following two equations:

$$
\begin{aligned}
N &= - \gamma^2 \frac{I_p}{Q} + I_0 + \int I(t) dt\\
N &= - (\gamma+r) \gamma \frac{I_p}{Q}
\end{aligned}
$$

This gives

$$
\begin{aligned}
(\gamma+r) \gamma \frac{I_p}{Q} &= \gamma^2 \frac{I_p}{Q} - I_0 - \int I(t) dt\\
\gamma^2 \frac{I_p}{Q} + r \gamma \frac{I_p}{Q} &= \gamma^2 \frac{I_p}{Q} - I_0 - \int I(t) dt\\
r \gamma \frac{I_p}{Q} &=- (I_0 + \int I(t) dt)\\
\end{aligned}
$$

```{r}
library("fitsir")

bombay2 <- setNames(bombay,c("tvec","count"))

iniP.ex <- startfun(data=bombay2, auto = TRUE)

findSens(bombay2, iniP.ex, plot.it = TRUE)

fitP <- coef(fitsir(bombay2, start = startfun(data=bombay2, auto = TRUE)))

findSens(bombay2, fitP, plot.it = TRUE)

```

```{r, error = FALSE}
(fitP2 <- fitsir.optim(bombay2, start = startfun(), plot.it=TRUE))

findSens(bombay2, fitP2, plot.it = TRUE)

##with auto start?

(fitP3 <- fitsir.optim(bombay2, start = startfun(data=bombay2, auto = TRUE), plot.it=TRUE))

findSens(bombay2, trans.pars(fitP3), plot.it = TRUE)

```

This is weird... We get lower sensitivity values but higher SSQ...

```{r }

(fitP4 <- fitsir.optim(bombay2, start = fitP, plot.it=TRUE))

findSens(bombay2, trans.pars(fitP4), plot.it = TRUE)

```

## Incidence

We need new equations for incidence... Let's assume that bombay2 data is incidence data. We're going to use $I(t)$ for incidence and $P(t)$ for prevalence from here.

```{r}

bombay2 <- setNames(bombay,c("tvec","count"))

fitP.inc <- coef(fitsir(bombay2, incidence = TRUE))

findSens(bombay2, fitP.inc, plot.it = TRUE, incidence = TRUE)

bombay3 <- data.frame(tvec = bombay2$tvec, count = cumsum(bombay2$count))

plot(bombay3)

lines(cumsum(SIR.detsim(bombay2$tvec, trans.pars(fitP.inc), incidence = TRUE)), col = 2)
```

I think I made incidence fitting to work but I still want to figure out the starting parameters.

```{r}

plot(SIR.detsim(1:800, trans.pars(fitP.inc), incidence = TRUE))

```

### Sensitivity

```{r inc.sens}
tvec <- seq(1, 50, 0.5)

trans.pars(pars)

fake.inc = data.frame(tvec, count = SIR.detsim(tvec, trans.pars(pars), incidence = TRUE))

plot(fake.inc)

fake.P <- coef(fitsir(fake.inc, incidence = TRUE))

trans.pars(fake.P)

findSens(fake.inc, fake.P, plot.it = TRUE, incidence = TRUE)
```

```{r}
(fake.P2 <- fitsir.optim(fake.inc, start = fake.P2, plot.it=TRUE, incidence = TRUE, verbose = TRUE))

findSens(fake.inc, fake.P2, plot.it = TRUE, incidence = TRUE)

```

This is weird... All three parameters are different but they give us similar incidence trajectory...

```{r}

r1 <- SIR.detsim(tvec, pars, reportAll = TRUE)
r2 <- SIR.detsim(tvec, trans.pars(fake.P), reportAll = TRUE)
r3 <- SIR.detsim(tvec, trans.pars(fake.P2), reportAll = TRUE)

plot(-diff(r1$S), type = "l")
lines(-diff(r2$S), col = 2)
lines(-diff(r3$S), col = 3)
```

### Auto-start

First, we know that $I(0)/ (t_1 - t_0) = \beta N (1-i_0) i_0 $, where $\Delta t$ is the reporting time interval.
```{r}
tvec <- fake.inc$tvec
count <- fake.inc$count

t.diff0 <- tvec[2] - tvec[1]

count[1] #5.19
with(as.list(pars),{beta * N * (1 -i0) * i0 * t.diff0}) ## 4.95
```

We also know that the the initial exponential rate of growth for $\int I(t) dt$ is $\beta$.

```{r}

ss <- smooth.spline(tvec,log(count),spar=0.5)
		## find max value
ss.tmax <- uniroot(function(x) predict(ss,x,deriv=1)$y,range(tvec))$root
		## find a point halfway between initial and max
		##  scaling could be adjustable?
ss.thalf <- min(tvec)+0.5*(ss.tmax-min(tvec))

ss.1 = floor(min(tvec) + 0.5 *(ss.thalf - min(tvec)))

cum.count = cumsum(count)

plot(tvec, log(count))
lines(predict(ss), col = 2)
abline(v = ss.1, lty = 2)

fake.inc <- transform(fake.inc, cum.count = cumsum(count))

m1 <- lm(log(cum.count)~tvec, data =subset(fake.inc, tvec < ss.thalf))

plot(tvec,log(fake.inc$cum.count))
lines(subset(fake.inc, tvec < ss.thalf)$tvec,predict(m1))

coef(m1)[2] ## 0.4087
pars[1] ## 0.5
```

Let's look at the derivative

$$
\begin{aligned}
\frac{dI}{dt} &= \beta/N (\frac{dS}{dt} P + S \frac{dP}{dt}) \\
&= \beta/N (-\beta S P^2/N + S (\beta S P/N - \gamma P)) \\
\end{aligned}
$$

When $dI/dt = 0$, we have

$$
\begin{aligned}
-\beta S P^2/N + S (\beta S P/N - \gamma P) &= 0 \\
-\beta P/N + \beta S /N - \gamma &= 0 \\
\beta/N (S - P) &= \gamma \\
\end{aligned}
$$

Let's see how accurate this is...

```{r testDeriv}
df.test <- SIR.detsim(tvec, pars, reportAll = TRUE)

ss.tmax.n = which(tvec == floor(ss.tmax))

attach(as.list(pars))
attach(df.test)
beta/N*(S[ss.tmax.n] - I[ss.tmax.n]) ##0.314
gamma ##0.3
detach(as.list(pars))
detach(df.test)

```

It seems fairly accurate.. Let's see if we can approximate it

First, we can approximate prevalence trajectory using the following equation: $P = I/(\gamma \Delta t)$. Substituting it in the equation given above: $\beta \gamma (N - I_0 - \int_0^{t_{max}} I d\tau) - \beta I_{max}/\Delta t = N\gamma^2$.

We can figure out all the

```{r findPrev}




```
