---
title: "Debugging fitsir problems"
author: Daniel Park and Ben Bolker
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
output: html_document
---

## Optim problem

```{r pkgs, message=FALSE}
library("deSolve")
library("bbmle")
library(lattice) ## for splom()
library(numDeriv)
## source("../R/fitSIR_funs.R")
## load *development* version of the package ...
library("devtools")
load_all("..")
## library("fitsir")
## set up names for consistency with fitsir
bombay2 <- setNames(bombay,c("tvec","count"))

#(fitP <- coef(fitsir(bombay2, start = startfun())))

#findSens(bombay2, fitP, plot.it = TRUE)
```

First, here is a set of parameters we get from the initial curve fittting attempt with \code{optim()}:
this is using `incidence=FALSE` and the new sensitivity machinery, but *not* using the new "autostart"
machinery (i.e. using the not-necessarily-sensible
default values of $\beta=0.12$, $\gamma=0.09$, $N=10^4$, $I(0)=0.01$), or

```{r params}
summarize.pars(p0 <- startfun())
p1 <- p0
p1$log.beta <- p1$log.beta + 0.1
p2 <- p0
p2$log.beta <- p2$log.beta + 0.2
p3 <- p0
p3$log.beta <- p2$log.beta + 0.3
pList <- list(p0,p1,p2,p3)
```

```{r SSQ_vs_NLL,message=FALSE}
## TO DO: get rid of attach()/detach() in findSens ...
SSQvec <- sapply(pList,findSens,data=bombay2,incidence=FALSE)["SSQ",]
tmpfun <- function(p) with(bombay2,SIR.logLik()(p,count,tvec))
nllvec <- sapply(pList,tmpfun)
plot(nllvec,SSQvec)
```

Hmmm.  I thought SSQ and negative log-likelihood should be perfectly linearly related, because `dnorm2` is defined as

$$
\begin{split}
\hat\sigma & = \sqrt{\sum{(x_i-\mu_i)^2/(n-1)}} \\
\cal L & = -n/2 \log(2 \pi) -n \log(\hat \sigma) - \sum((x_i-\mu_i)^2)/(2 \hat \sigma^2)
\end{split}
$$

Maybe they're not, but they should have their minima in the same place ... ??

The first time we iterate it, we get a large jump, but if we iterate the process few more times, we get a stable answer.

```{r sfits,cache=TRUE,message=FALSE}
## set up matrix for results
nsteps <- 8
npar <- 4
f0 <- unlist(startfun())
sfits <- matrix(NA,ncol=npar,nrow=nsteps,
               dimnames=list(NULL,names(f0)))
sfits[1,] <- f0  ## set first row
ffits <- sfits   ## setup for next step (see below)
for (i in 2:nsteps) {
  sfits[i,] <- fitsir.optim(bombay2, start=sfits[i-1,])
}
```

Now let's try the same thing with plain old `fitsir` (`dnorm2` instead of SSQ, Nelder-Mead
instead of BFGS + sensitivity equations)

```{r ffits,cache=TRUE}
for (i in 2:nsteps) {
  ffits[i,] <- coef(fitsir(bombay2, start=ffits[i-1,]))
}
```

```{r diffs,echo=FALSE}
par(las=1,bty="l")
matplot(ffits,col=1:4,type="l",lty=1,
        xlab="step",ylab="param value",
        xlim=c(1,10))
matlines(sfits,col=1:4,lty=2)
text(8,tail(ffits,1),colnames(ffits),col=1:4,adj=-1)
```

How different are the fits, and the implied likelihoods?
```{r compare_fits}
ffit.final <- ffits[nsteps,]
sfit.final <- sfits[nsteps,]
tmpfun(ffit.final)
tmpfun(sfit.final)
```

If we start with the `sfits` result do we get back to the `ffits` result
(and *vice versa*) ?
```{r reciprocal_start,cache=TRUE,message=FALSE}
## start fitsir from fitsir.optim best-fit
newfit <- fitsir(bombay2, start=sfit.final)
## start fitsir.optim from fitsir best-fit
newfit2 <- fitsir.optim(bombay2, start=ffit.final)
reldiff <- function(x,y) {
    max(abs((x-y)/((x+y)/2)))
}
print(cmat <- matrix(
    c(reldiff(sfit.final,coef(newfit)),
      reldiff(ffit.final,coef(newfit)),
      reldiff(sfit.final,newfit2),
      reldiff(ffit.final,newfit2)),
    nrow=2,
    dimnames=list(orig=c("fitsir","fitsir.optim"),
                  restart=c("fitsir.optim","fitsir"))),
    digits=3)
```
i.e., both methods get stuck when we start them at the alternative
location. Visualizing a "slice" along a line in parameter space (where 0 represents the place where the old fitsir gets stuck, 1 represents the solution found by `fitsir.optim`):

```{r slice1}
slice1 <- bbmle:::slicetrans(ffit.final,sfit.final,
                   tmpfun,
                   extend=0.2)
par(las=1,bty="l")
par(mfrow=c(1,3))
with(slice1$slices[[1]],
     plot(x,z,log="y",type="l"))
with(subset(slice1$slices[[1]],x<0.1),
     plot(x,z,log="y",type="l"))
with(subset(slice1$slices[[1]],x>0.9),
     plot(x,z,log="y",type="l"))
```

Looks like local maxima! Try full slices ... we probably won't be able
to see what's going on at the full scale (i.e. since the peak in the
middle is so huge), so let's just look at windows around

```{r slice2D,cache=TRUE}
## redo 'slice transect' computation
params <- ffit.final; params2 <- sfit.final
extend <- c(0.2,0.2); nt <- 401
cutoff <- 500
np <- length(params)
slicep <- sort(unique(c(0, 1, seq(-extend[1], 1 + extend[2], 
                                  length = nt))))
slicepars <- t(sapply(slicep, function(x) (1 - x) * params + 
                                          x * params2))
## select either end of the transect
s2Apars1 <- slicepars[slicep>(-0.2) & slicep<0.1,]
s2Apars2 <- slicepars[slicep>1 & slicep<1.2,]
## get first and last rows ...
s2Arange1 <- t(s2Apars1[c(1,nrow(s2Apars1)),])
s2Arange2 <- t(s2Apars2[c(1,nrow(s2Apars2)),])
s2A <- slice2D(sfit.final,
        tmpfun,
        tranges=s2Arange1,
        nt=41,verbose=FALSE)
## clean up crazy values: should incorporate in slice2D ...
s2Ax <- s2A
for (i in 1:3) {
    for (j in 1:4) {
        if (!is.null(s2Ax$slices[[i]][[j]])) {
            zz <- s2Ax$slices[[i]][[j]]$z
            s2Ax$slices[[i]][[j]]$z[zz>cutoff] <- NA
        }
    }
}
```

What's going on along the `log.beta`/`log.gamma` diagonal here?
Am I just being stupid?
(We should really restrict `log.beta>log.gamma` ... although I would
have thought that would just lead to bad fits?)

```{r persp3d,eval=FALSE}
library(rgl)
persp3d(1:41,1:41,matrix(s2Ax$slices[[1]][[2]]$z,41),
        col="blue",alpha=0.8,zlab="NLL")
```

OK, I'm giving up on trying to visualize the local region
for now.

Trying to compute local derivatives/Hessians isn't making sense
either ... ?

```{r localgrad,eval=FALSE}
grad(tmpfun,sfit.final,method="simple",method.args=list(eps=1e-2))
grad(tmpfun,ffit.final,method="simple",method.args=list(eps=1e-3))
h1 <- hessian(tmpfun,ffit.final)
h2 <- hessian(tmpfun,sfit.final)
```

How different are the trajectories?
```{r comp_traj}
straj <- SIR.detsim(bombay2$tvec,trans.pars(sfit.final))
ftraj <- SIR.detsim(bombay2$tvec,trans.pars(ffit.final))
with(bombay2,plot(tvec,count))
lines(straj)
lines(ftraj,lty=2)
```

How different are the parameter values from a more
epidemiological point of view (i.e. in terms of $R_0$, $r$, etc.)?

```{r summarize_pars}
print(summarize.pars(ffit.final),digits=3)
print(summarize.pars(sfit.final),digits=3)
```

Maybe the problem is caused by using the "wrong" parameterization?
Would we better off parameterizing via `log(R0)`, `log(r)` ... rather
than `log(beta)`, `log(gamma)`, ... ??

Do we do better (can we ignore this problem) if we use autostart ... ?

```{r autostart}
autofit.f <- fitsir(bombay2, start=startfun(auto=TRUE,data=bombay2))
## start fitsir.optim from fitsir best-fit
autofit.s <- fitsir.optim(bombay2, start=ffit.final)

## Startfun problem

We have the following four equations:

$$
\begin{aligned}
r &= \beta - \gamma \\
\frac{Q_p}{I_p} &= \frac{\beta \gamma}{N} \\
I(0) &= N i_0\\
\frac{\gamma N}{\beta} &= N - I(0) - \sum_0^{\tau_\text{peak}} I(t) dt
\end{aligned}
$$

Here is a made-up data:

```{r ex_sim}
pars <- list(
	beta = 0.5,
	gamma = 0.1,
	N = 500,
	i0 = 0.01
)
tvec <- bombay2$tvec
r = SIR.detsim(tvec, pars, reportAll = TRUE)
count = r$I
S = r$S
incid = c(NA,-diff(S))
data = data.frame(tvec, count)
```

I'm going to test the equations one by one...
$$
r = \beta - \gamma
$$

```{r}
attach(pars)
ncrit <- Inf
it <- 1
spar <- 0.5
while (ncrit>1 && it<10) {
	ss <- smooth.spline(tvec,log(count),spar=spar)
	dd <- predict(ss,deriv=1)$y
	ncrit <- sum(diff(sign(dd))!=0)
	spar <- (1+spar)/2
}
if (it==10) stop("couldn't smooth enough")
		## find max value
ss.tmax <- uniroot(function(x) predict(ss,x,deriv=1)$y,range(tvec))$root
		## find a point halfway between initial and max
		##  scaling could be adjustable?
ss.thalf <- min(tvec)+0.5*(ss.tmax-min(tvec))

m1 <- lm(log(count)~tvec,data=subset(data,tvec<ss.thalf))
(r <- as.numeric(coef(m1)[2])) ##beta - gamma

plot(log(count))
lines(predict(ss))
abline(v = ss.tmax, lty = 2)
abline(v = ss.thalf, lty = 2)
lines(predict(m1), col = 2)
```

0.37 is fairly close to 0.4 so this one is OK.

$$
Q_p = -\frac{\beta \gamma I_p}{N}
$$

```{r}
iniI <- count[1] ## N * i0
## curvature of spline at max: could also do this, possibly
##  more robustly, from a local quadratic fit
Qp.real = -beta*gamma*max(count)/N
Qp.alt <- predict(ss,ss.tmax,deriv=2)$y
(Q <- c(Qp.real=Qp.real, Qp.alt=Qp.alt))
```

This is OK too. For convenience, we're going to let $\alpha = \frac{\beta \gamma}{N} = -\frac{Q_p}{I_p}$. This is nice because we can approximate the right term. Here is the problem:

$$
\frac{\gamma N}{\beta} = \hat{S} = N - I(0) - \sum_0^{\tau_\text{peak}} I(t) dt$,
$$

where $I(t)$ is incidence

```{r}
plot(S)
abline(v = ss.tmax, lty = 2)
abline(h = gamma * N/beta, lty = 2)
```

The problem is that we can't approximate incidence from prevalence so well using this formula: $P = I/\gamma$

```{r}
plot(-diff(S), type = "l")
lines(count*gamma,lty=2,col=2)
```

This is a problem because this is what we want to do:

$$
\begin{aligned}
\frac{\gamma N}{\beta} &= N - I(0) - \sum_0^{\tau_\text{peak}} I(t) dt\\
\frac{\gamma N}{\beta} &= N - I(0) - \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
N(1 - \frac{\gamma}{\beta})  &= I(0) + \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
\frac{\beta\gamma}{\alpha} (1 - \frac{\gamma}{\beta})  &= I(0) + \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
\gamma \frac{r}{\alpha} &= I(0) + \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
\gamma (\frac{r}{\alpha} - \sum_0^{\tau_\text{peak}}  P(t) dt) &= I(0) \\
\gamma  &= I(0)/(\frac{r}{\alpha} - \sum_0^{\tau_\text{peak}}  P(t) dt)\\
\end{aligned}
$$

We test it with the predicted values

```{r}
Ip = max(exp(predict(ss)$y))

alpha = -Qp.alt/Ip

(new.gamma = iniI/(r/alpha - sum(count[1:ss.tmax])))

```

Here's is the real gamma value:

```{r}
iniI/(0.4/(-Qp.real/max(count)) - (N - 100 -iniI)/gamma)
```

In the equations above, we're assuming that $N  - I_0 - \sum (P(t) \gamma) = \hat{S}$ but it doesn't work so well...

```{r}
detach(pars)
```

Can we get anything from knowing the peak prevalence ($I_p$)
other than the calculation we've already done with $Q_p$?
The $dI/dt$ equation isn't useful (it just gives us
$\beta S = \gamma$, and we don't know $S$ without doing
the fancy stuff we're working for ...

```{r}
plot(incid,count,type="b",
     ylab="prevalence")
abline(a=0,b=1/gamma,lty=2)
