---
title: "Problems"
output: html_document
---

## Optim problem

```{r optim, error=FALSE}
library("deSolve")
library("bbmle")
## source("../R/fitSIR_funs.R")
## load *development* version of the package ...
library("devtools")
load_all("..")
## library("fitsir")
## set up names for consistency with fitsir
bombay2 <- setNames(bombay,c("tvec","count"))

#(fitP <- coef(fitsir(bombay2, start = startfun())))

#findSens(bombay2, fitP, plot.it = TRUE)
```

First, here is a set of parameters we get from the initial curve fittting attempt with \code{optim()}:
this is using `incidence=FALSE` and the new sensitivity machinery, but *not* using the new "autostart"
machinery (i.e. using the not-necessarily-sensible
default values of $\beta=0.12$, $\gamma=0.09$, $N=10^4$, $I(0)=0.01$), or

```{r params}
summarize.pars(p0 <- startfun())
p1 <- p0
p1$log.beta <- p1$log.beta + 0.1
p2 <- p0
p2$log.beta <- p2$log.beta + 0.2
p3 <- p0
p3$log.beta <- p2$log.beta + 0.3
pList <- list(p0,p1,p2,p3)
```

```{r SSQ_vs_NLL}
SSQvec <- sapply(pList,findSens,data=bombay2,incidence=FALSE)["SSQ",]
tmpfun <- function(p) with(bombay2,SIR.logLik()(p,count,tvec))
nllvec <- sapply(pList,tmpfun)
plot(nllvec,SSQvec)
```

Hmmm.  I thought SSQ and negative log-likelihood should be perfectly linearly related, because `dnorm2` is defined as

$$
\begin{split}
\hat\sigma & = \sqrt{\sum{(x_i-\mu_i)^2/(n-1)}} \\
\cal L & = -n/2 \log(2 \pi) -n \log(\hat \sigma) - \sum((x_i-\mu_i)^2)/(2 \hat \sigma^2)
$$

Maybe they're not, but they should have their minima in the same place ... ??

The first time we iterate it, we get a large jump, but if we iterate the process few more times, we get a stable answer.

```{r sfits}
## set up matrix for results
nsteps <- 8
npar <- 4
f0 <- unlist(startfun())
sfits <- matrix(NA,ncol=npar,nrow=nsteps,
               dimnames=list(NULL,names(f0)))
sfits[1,] <- f0  ## set first row
ffits <- sfits   ## setup for next step (see below)
for (i in 2:nsteps) {
  sfits[i,] <- fitsir.optim(bombay2, start=sfits[i-1,])
}
```

Now let's try the same thing with plain old `fitsir` (`dnorm2` instead of SSQ, Nelder-Mead
instead of BFGS + sensitivity equations)

```{r ffits,cache=TRUE}
for (i in 2:nsteps) {
  ffits[i,] <- coef(fitsir(bombay2, start=ffits[i-1,]))
}
```

```{r diffs,echo=FALSE}
par(las=1,bty="l")
matplot(ffits,col=1:4,type="l",lty=1,
        xlab="step",ylab="param value",
        xlim=c(1,10))
matlines(sfits,col=1:4,lty=2)
text(8,tail(ffits,1),colnames(ffits),col=1:4,adj=-1)
```

How different are the fits, and the implied likelihoods?
```{r}
tmpf <- SIR.logLik()
ffit.final <- ffits[nsteps,]
sfit.final <- sfits[nsteps,]
with(bombay2,tmpf(ffit.final,count,tvec))
with(bombay2,tmpf(sfit.final,count,tvec))  ## huge!
```

If we start with the `sfits` result do we get back to the `ffits` result?
```{r}
newfit <- fitsir(bombay2, start=sfit.final)
all.equal(sfit.final,coef(newfit),tol=5e-3)
all.equal(ffit.final,coef(newfit))
```

```{r}
straj <- SIR.detsim(bombay2$tvec,trans.pars(sfits[nsteps,]))
ftraj <- SIR.detsim(bombay2$tvec,trans.pars(ffits[nsteps,]))
with(bombay2,plot(tvec,count))
lines(straj)
lines(ftraj,lty=2)
```
Are we stuck at a local max?


I tried this (I set eval = FALSE) and the parameters keep on changing bit by bit...

```{r eval = FALSE}
(fitP11 <- coef(fitsir(bombay2, start = fitP10)))

(fitP12 <- coef(fitsir(bombay2, start = fitP11)))

(fitP13 <- coef(fitsir(bombay2, start = fitP12)))

(fitP14 <- coef(fitsir(bombay2, start = fitP13)))

(fitP15 <- coef(fitsir(bombay2, start = fitP14)))

(fitP16 <- coef(fitsir(bombay2, start = fitP15)))

(fitP17 <- coef(fitsir(bombay2, start = fitP16)))

(fitP18 <- coef(fitsir(bombay2, start = fitP17)))

(fitP19 <- coef(fitsir(bombay2, start = fitP18)))
```

Here's something interesting:

```{r}

plot(bombay2$count)

tvec = bombay2$tvec

lines(SIR.detsim(tvec, trans.pars(fitP2)))

lines(SIR.detsim(tvec, trans.pars(fitP5)), col = 2)

lines(SIR.detsim(tvec, trans.pars(fitP8)), col = 3)

fitP2
fitP5
fitP8
```

Something weird is happening...

## Startfun problem

We have the following four equations:

$$
\begin{aligned}
r &= \beta - \gamma\\
\frac{Q_p}{I_p} &= \frac{\beta \gamma}{N}\\
I(0) &= N i0\\
\frac{\gamma N}{\beta} &= N - I(0) - \sum_0^{\tau_\text{peak}} I(t) dt
\end{aligned}
$$

Here is a made-up data:

```{r ex,sim}

pars <- list(
	beta = 0.5,
	gamma = 0.1,
	N = 500,
	i0 = 0.01
)

r = SIR.detsim(tvec, pars, reportAll = TRUE)

count = r$I
S = r$S

data = data.frame(tvec, count)

```

I'm going to test the equations one by one...
$r = \beta - \gamma$

```{r}
attach(pars)
ncrit <- Inf
it <- 1
spar <- 0.5
while (ncrit>1 && it<10) {
	ss <- smooth.spline(tvec,log(count),spar=spar)
	dd <- predict(ss,deriv=1)$y
	ncrit <- sum(diff(sign(dd))!=0)
	spar <- (1+spar)/2
}
if (it==10) stop("couldn't smooth enough")
		## find max value
ss.tmax <- uniroot(function(x) predict(ss,x,deriv=1)$y,range(tvec))$root
		## find a point halfway between initial and max
		##  scaling could be adjustable?
ss.thalf <- min(tvec)+0.5*(ss.tmax-min(tvec))

m1 <- lm(log(count)~tvec,data=subset(data,tvec<ss.thalf))
(r <- as.numeric(coef(m1)[2])) ##beta - gamma
beta-gamma

plot(log(count))
lines(predict(ss))
abline(v = ss.tmax, lty = 2)
abline(v = ss.thalf, lty = 2)

lines(predict(m1), col = 2)

```

0.37 is fairly close to 0.4 so this one is OK.

$\frac{Q_p} = \frac{\beta \gamma I_p}{N}$

```{r}

iniI <- count[1] ## N * i0
	    ## curvature of spline at max
Qp.real = -beta*gamma*max(count)/N
Qp.alt <- predict(ss,ss.tmax,deriv=2)$y
(Q <- c(Qp.real=Qp.real, Qp.alt=Qp.alt))

```

This is OK too. For convenience, we're going to let $\alpha = \frac{\beta \gamma}{N} = \frac{Q_p}{I_p}$. This is nice because we can approximate the right term. Here is the problem:

$\frac{\gamma N}{\beta} = \hat{S} = N - I(0) - \sum_0^{\tau_\text{peak}} I(t) dt$, where $I(t)$ is incidence

```{r}

plot(S)
abline(v = ss.tmax, lty = 2)
abline(h = gamma * N/beta, lty = 2)

```

The problem is that we can't approximate incidence from prevalence so well using this formula: $P = I/\gamma$

```{r}
plot(-diff(S), type = "l")
lines(count * gamma)
```

This is a problem because this is what we want to do:

$$
\begin{aligned}
\frac{\gamma N}{\beta} &= N - I(0) - \sum_0^{\tau_\text{peak}} I(t) dt\\
\frac{\gamma N}{\beta} &= N - I(0) - \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
N(1 - \frac{\gamma}{\beta})  &= I(0) + \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
\frac{\beta\gamma}{\alpha} (1 - \frac{\gamma}{\beta})  &= I(0) + \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
\gamma \frac{r}{\alpha} &= I(0) + \gamma \sum_0^{\tau_\text{peak}}  P(t) dt\\
\gamma (\frac{r}{\alpha} - \sum_0^{\tau_\text{peak}}  P(t) dt) &= I(0) \\
\gamma  &= I(0)/(\frac{r}{\alpha} - \sum_0^{\tau_\text{peak}}  P(t) dt)\\
\end{aligned}
$$

We test it with the predicted values

```{r}
Ip = max(exp(predict(ss)$y))

alpha = -Qp.alt/Ip

(new.gamma = iniI/(r/alpha - sum(count[1:ss.tmax])))

```

Here's is the real gamma value:

```{r}
iniI/(0.4/(-Qp.real/max(count)) - (N - 100 -iniI)/gamma)
```

In the equations above, we're assuming that $N  - I0 - \sum (P(t) gamma) = \hat{S}$ but it doesn't work so well...

```{r}
detach(pars)
```

