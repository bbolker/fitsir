---
title: "Stochastic sims"
author: Ben Bolker
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r pkgs, message=FALSE}
library(fitsir)
library(bbmle) ## needed for coef() ...
library(splines) ## for ns()
library(plyr)  ## for raply()
library(dplyr)
library(tibble)
library(tidyr)
library(ggplot2); theme_set(theme_bw())
```

```{r stochsimfun}
source("stochsim_funs.R")
```

```{r ssex1}
set.seed(101)
s0 <- simfun(rpars=list(size=10))
```


Various ways of fitting ...
```{r fitex1,cache=TRUE}
## get starting values and trajectory based on them
## ss0 <- startfun(auto=TRUE,data=s0)
## use the generic (bad!) starting values
ss0 <- startfun()
ss2 <- SIR.detsim(s0$tvec,unlist(trans.pars(ss0)))

## fit and corresponding trajectory
t1 <- system.time(f1 <- fitsir(s0,start=ss0))
ss3 <- SIR.detsim(s0$tvec,trans.pars(coef(f1)))

## GAM fit: match number of degrees of freedom
## glm() lists df as df+2 

##   counting for intercept and residual var(?)
m1 <- glm(count~ns(tvec,df=3),
          family=gaussian(link="log"),data=s0)
```

`fitsir` works OK in this case even though we use the
generic starting parameters (not auto-fit, which is currently
broken):
```{r fitex1_plot,echo=FALSE}
par(las=1,bty="l")
plot(count~tvec,data=s0)
lines(s0$tvec,ss2)  ## incidence/prevalence mismatch?
lines(s0$tvec,ss3,col=2)  ## decent fit anyway
lines(s0$tvec,predict(m1,type="response"),col=4)
legend("topright",
       c("start","fitsir","spline"),
       col=c(1,2,4),lty=1)
```


## Preliminary ensemble results

```{r fitsim1,cache=TRUE}
## takes about 1 minute per sim ...
system.time(print(fitfun(s0)))
```
(autostart was used with these cached results ...)

```{r loadsims}
simfn <- "stochsim.rda"
if (file.exists(simfn)) {
   load(simfn)
} else {
set.seed(101)
res1 <- raply(20,fitfun(simfun(rpars=list(size=10))),
              .progress="text")
}
```

How well does this work?

```{r simhist,echo=FALSE}
par(las=1,bty="l")
with(res1,hist(nll.SIR-nll.gam,col="gray",breaks=20,
               main=""))
```

## Notes/preliminary conclusions

- these are fits to a reasonably well-behaved (although fairly noisy simulation example)
- with the current "autofit" functionality, `fitsir` mostly works OK, doesn't need Latin hypercube for this example
- in 20 sims, GAM/spline fit does slightly better most (90%) of the time, but not a big difference (<2 log-likelihood units)

```{r fit_try,cache=TRUE}
fitfun(s0)[c("nll.SIR","nll.gam")]
fitfun2(s0,plot.it=TRUE,log="y")
```

Note that in this case `fitsir` has *lower* mean-squared-error, despite having a higher negative log-likelihood. This makes some sense because `fitsir` appears better on the log scale ...

```{r sizecal,cache=TRUE}
sizevec <- 10^seq(0,2,length.out=20)
repvals <- 1:10
sres <- data.frame(expand.grid(size=sizevec,rep=repvals),
                   cbind(fitsir=NA,spline=NA))
set.seed(101)
for (i in 1:nrow(sres)) {
    sres[i,3:4] <- 
        fitfun2(simfun(rpars=list(size=sres[i,"size"])))
}
sres2 <- gather(sres,method,mse,-size,-rep)
ggplot(sres2,aes(size,mse,colour=method))+
    scale_x_log10()+
    scale_y_log10()+
    geom_point()+geom_smooth()+scale_colour_brewer(palette="Set1")
```

We may need pretty small `size` values to get match the observed range of mean-squared error values ...

A first crack at comparing `fitsir` and spline (three ways to do this -- (1) true start values; (2) generic start values; (3) LHS of starting values, pick best)

```{r loadsims2}
simfn2 <- "stochsim2.rda"
fitfun.optim <- function(data) {
    t1 <- system.time(f1 <- fitsir.optim(data,start=startfun(auto=TRUE,data=data)))
    m1 <- glm(count~ns(tvec,df=3),family=gaussian(link="log"),data=data)
    res <- c(t=unname(t1["elapsed"]),coef(f1),
      nll.SIR=c(-logLik(f1)),nll.gam=c(-logLik(m1)))
    return(res)
}

if (file.exists(simfn2)) {
   load(simfn2)
} else {
    set.seed(101)
    res1 <- raply(20,fitfun(simfun(rpars=list(size=10))))
    save("res1",file=simfn2)
}
```

## Ranges

from Dora:

```{r}
Nquant <- setNames(
    c(9.908985e+00,4.492488e+02,2.096015e+03,2.878854e+04,2.865343e+31),
    seq(0,100,by=25))
I0quant <-  setNames(
    c(6.573213e-193,6.255714e-04,9.216386e-03,4.933392e-02,9.990980e-01),
    seq(0,100,by=25))

m0 <- matrix(c(1.16,1.66,3.85,3.98,7.68,15.08,0.16,0.36,0.57,0.07,0.13,0.25,
         1.12,1.34,2.11,3.61,6.85,11.07,0.15,0.28,0.55,0.09,0.14,0.28,
         1.26,2.27,5.13,3.90,7.19,11.49,0.26,0.47,1.08,0.09,0.14,0.26,
         1.24,1.88,8.60,4.71,12.97,42.06,0.16,0.31,0.73,0.02,0.08,0.21),
         ncol=4)
dimnames(m0) <- list(paste(rep(c("R0","1/gamma","beta","gamma"),each=3),
                     rep(paste0("Q",1:3),4),sep="_"),
                     c("GB","BR","FI","ID"))
m1 <- m0 %>% as.data.frame %>% rownames_to_column("var") %>%
    separate(var,c("var","quantile"),sep="_") %>%
    gather(country,value,-c(var,quantile))
## mutate(qq=as.numeric(gsub("Q","",quantile))*0.25)
ggplot(m1,aes(country,value))+geom_point()+
    facet_wrap(~var,scale="free")
```

- compare `smooth.spline` and `glm(.,ns(.))` approaches
- get parameter ranges from DR: $\beta$, $\gamma$, $N$, $I(0)$, and number of data points (for simplicity we will take the same overall time range, and use number of data points for `dt`)
- calibrate neg binomial size parameter to observed mean squared error
- larger sample
- parameters more typical of DR/music-download data: esp. more samples (will presumably make differences *more* significant/favour GAMs more?)
- try factorial experiment:
    - multiple sims
    - range of sample sizes
    - range of true parameter values (Latin hypercube??)

## comparing smooth.spline and `lm(y~ns())`

**to do/update**

- it seems that a fairer matching of model complexity would use `df=4`/`nknots=2`: `?splines::ns` says "One can supply ‘df’ rather than knots; ‘ns()’ then chooses ‘df - 1 - intercept’ knots ...". However, `nknots=2` seems to *break* `smooth.spline()`, which does some fancy internal calculations to compute the smoothing parameter (see `?smooth.spline` ...)
- compute the negative log-likelihood for spline fits: these have to be adjusted (by adding $\sum 1/y$??) to allow for change of scale if we fit the splines on the log scale and compute the likelihood of the SIR model on the linear scale (which raises the question why we're doing that in the first place ...)

```{r fitcomp,cache=TRUE}
bombay2 <- setNames(bombay,c("tvec","count"))
f1 <- fitsir(bombay2,start=startfun(auto=TRUE,data=bombay2))
fpred1 <- SIR.detsim(bombay2$tvec,unlist(trans.pars(coef(f1))))
s1 <- lm(log(count)~ns(tvec,df=6),data=bombay2)
s2 <- with(bombay2,smooth.spline(tvec,log(count),nknots=4))
s3 <- lm(log(count)~ns(tvec,df=4),data=bombay2)
try(with(bombay2,smooth.spline(tvec,log(count),nknots=2)))
plot(count~tvec,data=bombay2,log="y")
with(bombay2,lines(tvec,exp(predict(s1)),col=2))
with(bombay2,lines(tvec,exp(fitted(s2)),col=4))
with(bombay2,lines(tvec,exp(fitted(s3)),col=5))
with(bombay2,lines(tvec,fpred1,col=1))
```

## stoch sim batch results

```{r ss2,echo=FALSE}
load("stochsim_4.rda")  ## main results
cat("total number run:",tail(which(!is.na(res[,1])),1),"\n")  ## 85
res <- na.omit(res)
cat("total number successful:",nrow(res),"\n") ## 78; 8 autostarts failed
res2 <- res  %>% as.data.frame %>% select(nll.SIR:mse.spline2) %>%
    gather() %>% separate(key,c("stat","method"),sep="\\.") %>%
    filter(stat!="nll") ## spline NLLs need to be scale-corrected
ggplot(res2,aes(log10(value),fill=method,colour=method))+
    geom_histogram(position="identity",alpha=0.5,bins=30)+
    scale_colour_brewer(palette="Set1")+
    scale_fill_brewer(palette="Set1")
```
